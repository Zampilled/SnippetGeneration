raw
"class AdminOrderAPI(ListAPIView):\n    permission_classes = [\n        permissions.IsAdminUser\n    ]\n    serializer_class = OrderSerializer\n    def get_queryset(self):\n        return Order.objects.filter(sent=False)\n\n\nclass OrderAPI(ListAPIView):\n    permission_classes = [\n        permissions.IsAuthenticated\n    ]\n    serializer_class = OrderSerializer\n    def get_queryset(self):\n        return Order.objects.filter(owner=self.request.user, received=False)"
"class SendOrderAPI(generics.GenericAPIView):\n    permission_classes = [\n        permissions.IsAdminUser\n    ]\n    serializer_class = CartSerializer\n\n    def post(self, request):\n        serializer = SendSerializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        data = serializer.validated_data\n        id = data['id']\n        orderqs = Order.objects.filter(id=id)\n        if orderqs.exists():\n            order = orderqs[0]\n            if order.sent == False & order.received == False:\n                order.sent = True\n                order.save()\n                return Response({\n                    'status': 'Order sent'\n                },status=status.HTTP_200_OK)\n            else:\n                return Response({\n                    'status': 'Order Already Sent'\n                },status=status.HTTP_400_BAD_REQUEST)\n        else:\n            return Response({\n                'status': 'Order Doesnt Exist'\n            })"
"class OrderRecievedAPI(generics.GenericAPIView):\n    permission_classes = [\n        permissions.IsAuthenticated\n    ]\n    serializer_class = OrderSerializer\n    def post(self, request):\n        serializer = SendSerializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        data = serializer.validated_data\n        id = data['id']\n        orderqs = Order.objects.filter(id=id, owner=request.user)\n        if orderqs.exists():\n            order = orderqs[0]\n            if order.received == False:\n                order.received = True\n                order.save()\n                return Response({\n                    'status': 'Order Received'\n                },\n                    status=status.HTTP_200_OK)\n            else:\n                return Response({\n                    'status': 'Order Already Received or Not Sent'\n                },\n                    status=status.HTTP_400_BAD_REQUEST)\n        else:\n            return Response({\n                'status': 'Order Doesnt Exist'\n            })"
class CartAPI(ListAPIView):\n    permission_classes = [\n        permissions.IsAuthenticated\n    ]\n    serializer_class = CartSerializer\n\n    def get_queryset(self):\n        return Cart.objects.filter(owner=self.request.user)
"#Add To Cart\nclass UpdateCartAPI(generics.GenericAPIView):\n    permission_classes = [\n        permissions.IsAuthenticated,\n    ]\n\n    serializer_class = CartItemSerializer\n\n    def patch(self, request):\n        serializer = AddProductSerializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        data = serializer.validated_data\n        product = get_object_or_404(Product, pk=data['id'])\n        quantity = data['quantity']\n\n        cart_qs = Cart.objects.filter(owner=self.request.user)\n        if cart_qs.exists():\n            cart = cart_qs[0]\n            if cart.products.filter(product__pk=product.pk).exists():\n                if quantity <= 0:\n                    item = cart.products.get(product = product.pk)\n                    item.delete()\n                else:\n                    item = cart.products.get(product = product.pk)\n                    item.quantity = quantity\n                    item.save()\n                    return Response({""message"": ""Quantity Updated""\n                                     },\n                                    status=status.HTTP_200_OK\n                                    )\n            else:\n                return Response({""message"": ""Product doesnt exist"",\n                                 },\n                                status=status.HTTP_400_BAD_REQUEST\n                                )\n\n    def post(self, request):\n        serializer = AddProductSerializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        data = serializer.validated_data\n        product = get_object_or_404(Product, pk=data['id'])\n        quantity = data['quantity']\n        cart_qs = Cart.objects.filter(owner=self.request.user)\n        if cart_qs.exists():\n            cart = cart_qs[0]\n            if cart.products.filter(product__pk=product.pk).exists():\n                item = cart.products.get(product = product.pk)\n                item.quantity += quantity\n                item.save()\n\n                return Response({""message"": ""Quantity Added"",\n                                 },\n                                status=status.HTTP_200_OK\n                                )\n            else:\n                cart.products.create(\n                    product=product,\n                    owner=self.request.user,\n                    quantity=quantity,\n                    name=product.name,\n                    price=product.price,\n                    image=product.image,\n                    description=product.description,\n                )\n                return Response({""message"": "" Item added to your cart"", },\n                                status=status.HTTP_200_OK,\n                                )\n        else:\n\n            cart = Cart.objects.create(owner=self.request.user)\n            cart.products.create(\n                product=product,\n                owner=self.request.user,\n                quantity=quantity,\n                name=product.name,\n                price=product.price,\n                image=product.image,\n                description=product.description,\n            )\n            return Response({""message"": ""Order is created & Item added to your cart"", },\n                            status=status.HTTP_200_OK,)\n\n    def put(self, request, *args, **kwargs):\n        serializer = AddProductSerializer(data=request.data)\n        serializer.is_valid()\n        data = serializer.validated_data\n        product = CartItem.objects.filter(product=data['id'])\n        product.delete()\n        return Response({\n            ""id"": data['id'],\n        },status=status.HTTP_200_OK,)"
"class CheckoutAPI(generics.GenericAPIView):\n    permission_classes = [\n        permissions.IsAuthenticated\n    ]\n    serializer_class = CartItemSerializer\n\n    def post(self, request):\n        serializer = CheckoutSerializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        data = serializer.validated_data\n        cart_qs = Cart.objects.filter(owner=self.request.user)\n        cartitem_qs = CartItem.objects.filter(owner=self.request.user)\n        if cartitem_qs.exists():\n            if cart_qs.exists():\n                cart = cart_qs[0]\n                order = Order.objects.create(\n                    owner=cart.owner,\n                    payment=data['payment'],\n                    delivery=data['delivery'],\n                    total=cart.total\n                )\n                i=0\n                while i<cart.products.all().count():\n                    order.products.create(\n                        productId=cart.products.all()[i].product.pk,\n                        image=cart.products.all()[i].image,\n                        name=cart.products.all()[i].name,\n                        quantity=cart.products.all()[i].quantity\n                    )\n                    i+=1\n                order.save()\n                cart.delete()\n                cartitem_qs.delete()\n                Cart.objects.create(owner=self.request.user)\n                return Response({'status': 'Checkout Complete'},\n                                status=status.HTTP_200_OK)\n            else:\n                Cart.objects.create(owner=self.request.user)\n                return Response({'status': 'Cart Doesnt Exist. Creating One.'},\n                                status=status.HTTP_400_BAD_REQUEST)\n        else:\n            return Response({'status': 'Cart Is Empty'}, status=status.HTTP_400_BAD_REQUEST)"
"class ProductViewSet(viewsets.ModelViewSet):\n\n    serializer_class = ProductSerializer\n\n    def get_permissions(self):\n        if self.request.method == 'GET':\n            self.permission_classes = [permissions.IsAuthenticated]\n        else:\n            #Replace text surrounded with # with self.permission_classes = [permissions.IsAdminUser]\n            #This code is only to preserve the demo webside\n            #\n\n            if self.request.user.username == 'zampilled':\n                self.permission_classes = [permissions.IsAdminUser]\n            else:\n                self.permission_classes = [permissions.NOT]\n            #\n        return super(ProductViewSet, self).get_permissions()\n\n    def get_queryset(self):\n        return Product.objects.all()"
"import ast\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass EncodedDataset(Dataset):\n    """"""\n\n    The EncodedDataset class is a subclass of Dataset. It represents a dataset with encoded text and reaction\n    information. It is intended to be used for training and evaluating reaction encoder models.\n\n    Attributes:\n        text (list): The original text data.\n        reaction (list): The reaction data associated with each text.\n        count (list): The count data associated with each text.\n        encoded_text (list): The encoded representation of the text data.\n        encoded_reaction (list): The encoded representation of the reaction data.\n\n    Methods:\n        __len__(self):\n            Returns the length of the dataset, which is the number of text samples.\n\n        __getitem__(self, index): Returns a dictionary containing the text, reaction, encoded_text, encoded_reaction,\n        and count for the specified index.\n\n    Usage example:\n        dataset = EncodedDataset(text, reaction, count, encoded_text, encoded_reaction)\n        data = dataset[0]\n        print(data)\n\n    Output:\n        {\n            'text': 'example text',\n            'reaction': 'example reaction',\n            'encoded_text': torch.tensor([1, 2, 3, 4], dtype=torch.float),\n            'encoded_reaction': torch.tensor([5, 6, 7, 8], dtype=torch.float),\n            'count': 10\n        }\n\n    """"""\n\n    def __init__(self, text, reaction, count, encoded_text, encoded_reaction):\n        self.text = text\n        self.reaction = reaction\n        self.count = count\n        self.encoded_text = encoded_text\n        self.encoded_reaction = encoded_reaction\n        self.train = False\n\n    def __len__(self):\n        return len(self.text)\n\n    def set_train(self, train):\n        self.train = train\n\n    def __getitem__(self, index):\n        if self.train:\n            return {\n                'encoded_text': torch.tensor(self.encoded_text[index], dtype=torch.float),\n                'encoded_reaction': torch.tensor(self.encoded_reaction[index], dtype=torch.float),\n            }\n        else:\n            return {\n                'text': self.text[index],\n                'reaction': self.reaction[index],\n                'count': self.count[index],\n                'encoded_text': torch.tensor(self.encoded_text[index], dtype=torch.float),\n                'encoded_reaction': torch.tensor(self.encoded_reaction[index], dtype=torch.float),\n            }\n"
"def create_dataset(rawFile, encoded_text, encoded_reaction):\n    """"""Create a dataset using the provided raw file and encoded text and reaction.\n\n    :param rawFile: The path to the raw file.\n    :type rawFile: str\n    :param encoded_text: The encoding for the text data.\n    :type encoded_text: str\n    :param encoded_reaction: The encoding for the reaction data.\n    :type encoded_reaction: str\n    :return: The created dataset.\n    :rtype: EncodedDataset\n    """"""\n    print(""Importing Data from {}"".format(rawFile))\n    raw = pd.read_csv(rawFile)\n    raw[""reaction""] = raw[""reaction""].apply(ast.literal_eval)\n    raw[""count""] = raw[""count""].apply(ast.literal_eval)\n    my_dataset = EncodedDataset(raw['text'], raw['reaction'], raw[""count""], encoded_text, encoded_reaction)\n    print(""Dataset Successfully Created"")\n    torch.save(my_dataset, (rawFile[:-4] + ""_encoded.pt""))"
"import ast\n\nimport gensim.models as gen\nimport pandas as pd\n\n\ndef create_word2vec(EMBEDDING_SIZE):\n    """"""\n    Create and train a Word2Vec model using the provided size for word embeddings.\n\n    :param EMBEDDING_SIZE: Integer specifying the size of word embeddings.\n    :return: None\n    """"""\n\n    # Importing Data\n    print(""Importing Data"")\n    df = pd.read_csv(""train.csv"")\n    train_reaction = df[""reaction""]\n    df2 = pd.read_csv(""test.csv"")\n    test_reaction = df2[""reaction""]\n    df3 = pd.read_csv(""valid.csv"")\n    val_reaction = df3[""reaction""]\n\n    processed = [row for row in pd.concat([train_reaction, test_reaction, val_reaction]).apply(ast.literal_eval)]\n    print(""Imported"", len(processed), ""Rows of Data"")\n\n    # Create Word2Vec Reaction Model\n    print(""Creating Word2Vec Reaction Model"")\n    my_model = gen.Word2Vec(min_count=1, window=10, vector_size=EMBEDDING_SIZE)\n    my_model.build_vocab(processed)\n    my_model.train(processed, total_examples=len(train_reaction.index), epochs=30)\n    print(""Word2Vec Model has"", len(my_model.wv.index_to_key), ""Keys"")\n    my_model.wv.save(""word2vec.wordvectors"")"
"\nimport numpy as np\nimport pandas as pd\nfrom langdetect import detect\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n\ndef languageDetector(row):\n    try:\n        return detect(row)\n    except:\n        return ""null""\n\n\ndef preprocess(inputPath):\n    """"""\n    Basic preprocessing of data given the input path (Input Path).\n    Creates two files (test.csv and train.csv) which are the preprocessed data.\n\n    :param inputPath: input path of raw data\n    :return: null\n    """"""\n\n    # Importing and Splitting Data\n    print(""Importing and Splitting Data"")\n    reactions_df = pd.read_json(inputPath)\n    top_reactions = []\n    reactions = reactions_df[""reactions""]\n    df2 = pd.DataFrame(\n        {""reaction"": np.zeros_like(reactions), ""count"": np.zeros_like(reactions)})\n\n    # Building Reaction Array\n    print(""Building Reaction Array"")\n\n    reaction_list = [""SAD"", ""WOW"", ""LOVE"", ""HAHA"", ""ANGRY"", ""THANKFUL""]\n    reaction_list_small = [""SAD"", ""WOW"", ""LOVE"", ""HAHA"", ""ANGRY""]\n\n    for idx, row in tqdm(enumerate(reactions)):\n        if len(row) > 6:\n            tempReaction = reaction_list\n        else:\n            tempReaction = reaction_list_small\n        for reaction in tempReaction:\n            if row[reaction] > 0:\n                temp = df2[""reaction""][idx]\n                temp_c = df2[""count""][idx]\n                if temp != 0:\n                    temp.append(reaction)\n                    temp_c.append(row[reaction])\n                else:\n                    temp = [reaction]\n                    temp_c = [int(row[reaction])]\n\n                df2[""reaction""][idx] = temp\n                df2[""count""][idx] = temp_c\n\n    df = pd.DataFrame({""text"": reactions_df[""message""], ""timestamp"": reactions_df[""date""]})\n    df = pd.concat([df, df2], axis=1, join=""inner"")\n    df.drop(df[df[""reaction""] == 0].index, inplace=True)\n\n    # Making text lower case and Removing non-English Words\n    print(""Making Text Lower Case and Removing Non-English Words"")\n\n    df[""text""] = df[""text""].str.lower()\n    df[""lang""] = df[""text""].apply(languageDetector)\n    df.drop(df[df[""lang""] != ""en""].index, inplace=True)\n    df.drop(""lang"", axis=1, inplace=True)\n    df.drop(df[df[""text""] == "" ""].index, inplace=True)\n\n    df.sort_values(by=""timestamp"", inplace=True)\n\n    # Exporting Processed Data\n    print(""Exporting Processed Data"")\n    df.to_csv(""processed.csv"")\n"
"def getVectors(row, count, model, EMBEDDING_SIZE):\n    """"""\n    Calculate the average vector representation for a given row.\n\n    :param row: List of indices for the elements in the row.\n    :type row: list\n    :param count: List of occurrence counts for each element in the row.\n    :type count: list\n    :param model: Dictionary containing embeddings for each element.\n    :type model: dict\n    :param EMBEDDING_SIZE: Size of the embedding vector for each element.\n    :type EMBEDDING_SIZE: int\n    :return: Average vector representation for the given row.\n    :rtype: numpy.ndarray\n    """"""\n    total_vector = np.zeros([EMBEDDING_SIZE])\n    for x in range(len(row)):\n        total_vector += model[row[x]] * count[x]\n    return total_vector / sum(count)"
"def getReactions(inputReactions, count, model, EMBEDDING_SIZE):\n    """"""\n    Get processed reactions based on input reactions, count, model, and embedding size.\n\n    :param inputReactions: A pandas DataFrame containing the input reactions.\n    :param count: A list containing the count for each input reaction.\n    :param model: The model used for processing the reactions.\n    :param EMBEDDING_SIZE: The size of the reaction embedding.\n    :return: A list of processed reactions.\n\n    """"""\n    processed = [row for row in inputReactions.apply(ast.literal_eval)]\n    for x in range(len(processed)):\n        processed[x] = getVectors(processed[x], count[x], model, EMBEDDING_SIZE)\n    return processed"
"def get_reaction_embeddings(inputFile, EMBEDDING_SIZE):\n    """"""\n    Perform word encoding of reactions in a given file.\n\n    :param inputFile: Path to the input file containing reaction data.\n    :type inputFile: str\n    :param EMBEDDING_SIZE: Size of the word embedding vector.\n    :type EMBEDDING_SIZE: int\n    :return: A list of reaction embeddings.\n    :rtype: list\n    """"""\n    print(""Beginning Reaction Encoding for file"", inputFile)\n\n    model = gen.KeyedVectors.load(""word2vec.wordvectors"", mmap='r')\n    # Importing Data\n    print(""Importing Data"")\n    df = pd.read_csv(inputFile)\n\n    df[""count""] = df[""count""].apply(ast.literal_eval)\n    print(""Finished Reaction Encoding for file"", inputFile)\n    return getReactions(df[""reaction""], df[""count""], model, EMBEDDING_SIZE)"
"\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_data(fileName, split, SEED):\n    """"""\n    Splits the data in the given file into training, testing, and validation datasets and exports them as CSV files.\n\n    :param fileName: str - The path to the input file.\n    :param split: tuple - A tuple of two floats representing the proportion of\n    test and validation data respectively (0-1)\n    :param SEED: int - The random\n    seed for reproducible results.\n\n    :return: None\n    """"""\n    df = pd.read_csv(fileName)\n\n    # Create training and other data\n    training_data, temp = train_test_split(df, test_size=sum(split), random_state=SEED)\n\n    # Calculate test split based on original dataframe size\n    test_split = split[1] / sum(split)\n\n    # Create testing and validation data\n    testing_data, validation_data = train_test_split(temp, test_size=test_split, random_state=SEED)\n\n    training_data.reset_index(drop=True, inplace=True)\n    testing_data.reset_index(drop=True, inplace=True)\n    validation_data.reset_index(drop=True, inplace=True)\n\n    # Exporting Processed Data\n    print(""Exporting Processed Data"")\n    training_data.to_csv(""train.csv"")\n    testing_data.to_csv(""test.csv"")\n    validation_data.to_csv(""valid.csv"")\n"
"def mse(historical_distribution, pred_distribution):\n    """"""\n    Calculate the Mean Squared Error (MSE) between the historical distribution and the predicted distribution.\n\n    :param historical_distribution: The historical distribution of values.\n    :param pred_distribution: The predicted distribution of values.\n    :return: The MSE value.\n    """"""\n    return mean_squared_error(historical_distribution, pred_distribution)"
"def get_test_data(model, word2vec, testing_set):\n    """"""\n    Run test data through encoder.\n\n    :param model: The model used for prediction.\n    :param word2vec: The Word2Vec model.\n    :param testing_set: The dataset used for testing.\n    :return: The predictions and corresponding reactions.\n    """"""\n    with torch.no_grad():\n        prediction = []\n        for x in range(len(testing_set)):\n            prediction.append(model(testing_set[x][""encoded_text""]).cpu().numpy()[0])\n        prediction_reaction = [(word2vec.similar_by_vector(x, topn=7)) for x in prediction]\n\n    return prediction, prediction_reaction"
"def get_predicted_distribution(reactions):\n\n    """"""\n    Calculate the distribution of the predicted reactions.\n\n    :param reactions: list of lists representing reaction data.\n                      Each inner list represents a row of reaction data.\n                      Each element in the inner list is a tuple of reaction name and count.\n    :return: list of lists representing the distribution of predicted reactions.\n             Each inner list represents a row of reaction distribution.\n             Each element in the inner list is the normalized count of a reaction.\n    """"""\n    reactionList = [""SAD"", ""WOW"", ""LOVE"", ""HAHA"", ""ANGRY"", ""THANKFUL""]\n    reaction_distribution = []\n    for row in tqdm(reactions):\n        row_count = [x[1] for x in row]\n        reaction_distribution.append(torch.softmax(torch.tensor(row_count), dim=0))\n    return reaction_distribution"
"def get_historical_distribution(testing_set):\n    """"""\n    Calculate the distribution of historical reactions for each testing set.\n\n    :param testing_set: A list of dictionaries representing testing data.\n                        Each dictionary contains two keys:\n                        - ""reaction"": A list of reactions.\n                        - ""count"": A list of the corresponding reaction counts.\n\n    :return: A list of lists representing the historical distribution of reactions.\n             Each inner list contains the normalized reaction counts for each reaction\n             in the reactionList.\n    """"""\n    reactionList = [""SAD"", ""WOW"", ""LOVE"", ""HAHA"", ""ANGRY"", ""THANKFUL""]\n    historical_data = []\n    for x in range(len(testing_set)):\n        row_historical_data = []\n        total = sum(testing_set[x][""count""])\n        for reaction in reactionList:\n            reaction_exits = False\n            for i in range(len(testing_set[x][""reaction""])):\n                if reaction == testing_set[x][""reaction""][i]:\n                    row_historical_data.append(testing_set[x][""count""][i] / total)\n                    reaction_exits = True\n            if not reaction_exits:\n                row_historical_data.append(0)\n\n        historical_data.append(row_historical_data)\n    return historical_data"
"def test_model(inputFile, EMBEDDING_SIZE):\n    """"""\n    Test the model using the given input file and embedding size.\n\n    :param inputFile: The path to the input file containing the testing set.\n    :param EMBEDDING_SIZE: The size of the embedding.\n    :return: None\n\n    Example usage:\n        >>> test_model('test_data.pt', 100)\n    """"""\n    print(""Cuda:"", torch.cuda.is_available())\n    print(""Device:"", torch.cuda.current_device())\n    print(""Device Name:"", torch.cuda.get_device_name(torch.cuda.current_device()))\n    device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n    model = LinearClass(EMBEDDING_SIZE).to(device)\n    model.load_state_dict(torch.load(""mymodel.pth"", map_location=torch.device(device)))\n    word2vec = gen.KeyedVectors.load(""word2vec.wordvectors"", mmap='r')\n    testing_set = torch.load(inputFile, map_location=torch.device(device))\n\n    pred_vector, pred_reaction = get_test_data(model, word2vec, testing_set)\n\n    # Getting distribution of Predicted Reactions\n    print(""Getting distribution of Predicted Reactions"")\n    pred_distribution = get_predicted_distribution(pred_reaction)\n\n    # Getting distribution of Historical Reactions\n    print(""Getting distribution of Historical Reactions"")\n    historical_distribution = get_historical_distribution(testing_set)\n\n    print(""MSE:"", mse(historical_distribution, pred_distribution))"
"class TextDataset(Dataset):\n    """"""\n\n    :class: TextDataset\n\n    The TextDataset class is a custom dataset class that is used for representing text data. It inherits from the\n    PyTorch Dataset class.\n\n    Methods:\n    ---------\n\n    __init__(self, inputs, device)\n        Initializes an instance of the TextDataset class.\n\n        Parameters:\n        -----------\n        inputs : dict\n            A dictionary containing the input data. It should have the following keys:\n            - 'input_ids': A list of input ids for the text data.\n            - 'attention_mask': A list of attention masks for the text data.\n        device : torch.device\n            The device on which to store the tensors.\n\n    __len__(self)\n        Returns the length of the dataset.\n\n        Returns:\n        --------\n        int\n            The length of the dataset.\n\n    __getitem__(self, index)\n        Returns an item from the dataset at a specified index.\n\n        Parameters:\n        -----------\n        index : int\n            The index of the item to retrieve.\n\n        Returns:\n        --------\n        dict\n            A dictionary containing the following keys:\n            - 'input_ids': The input ids for the text data at the specified index.\n            - 'attention_mask': The attention mask for the text data at the specified index.\n    """"""\n    def __init__(self, inputs, device):\n        self.ids = torch.tensor(inputs['input_ids'], dtype= torch.long).to(device)\n        self.masks = torch.tensor(inputs['attention_mask'], dtype= torch.long).to(device)\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        return {\n            ""input_ids"": self.ids[index],\n            ""attention_mask"": self.masks[index],\n        }"
"def getEmbeddings(df, tokenizer, MAX_LEN, BATCH_SIZE, device, bert):\n    """"""\n    Encodes text using pre-trained BERT encoder\n\n    :param df: Input Text Series\n    :param tokenizer: Bert Tokenizer\n    :param MAX_LEN: Maximum Token Length\n    :param BATCH_SIZE: Batch size of training\n    :param device: device to run on\n    :param bert: pre-trained BERT to use\n    :return: Encoded Text\n    """"""\n    # Tokenizing Text\n    print(""Tokenizing Text"")\n    inputs = tokenizer.batch_encode_plus(\n        df,\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding=""max_length"",\n        truncation=True,\n\n    )\n\n    # Encoding Text\n    print(""Encoding Text"")\n    with torch.no_grad():\n        outputs = []\n        bert.eval()\n        text_dataset = TextDataset(inputs, device)\n        text_dataloader = DataLoader(text_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n        for _, data in tqdm(enumerate(text_dataloader, 0)):\n            ids = data['input_ids'].to(device, dtype=torch.long)\n            mask = data['attention_mask'].to(device, dtype=torch.long)\n\n            _, output = bert(ids, attention_mask=mask, return_dict=False)\n            outputs += output.cpu()\n        print(""Done Encoding Text"")\n        return outputs\n"
"def get_text_embeddings(inputFile, MAX_LEN, BATCH_SIZE):\n    """"""\n    :param inputFile: The input file path containing the text data to be encoded.\n    :param MAX_LEN: The maximum length of the input text sequences.\n    :param BATCH_SIZE: The batch size used for encoding the text data.\n    :return: The encoded text embeddings as a pandas Series.\n    """"""\n\n    print(""Beginning Text Encoding for file"", inputFile)\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    print(""Using Device :"", torch.cuda.get_device_name(torch.cuda.current_device()))\n    device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n    bert = BertModel.from_pretrained(""bert-base-uncased"").to(device)\n\n    # Importing Data\n    print(""Importing Data"")\n    df = pd.read_csv(inputFile)\n    df[""text""] = df[""text""].astype(str)\n\n    df[""text""] = getEmbeddings(\n        df=df[""text""],\n        tokenizer=tokenizer,\n        MAX_LEN=MAX_LEN,\n        BATCH_SIZE=BATCH_SIZE,\n        device=device,\n        bert=bert\n    )\n    # Saving Dataset\n    print(""Outputting Encoded Text"")\n    return df[""text""]"
"class LinearClass(nn.Module):\n    """"""\n    LinearClass\n\n    A class representing a linear neural network with ReLU activation.\n\n    Usage:\n    ------\n\n    Instantiate the LinearClass object by providing the desired embedding size:\n\n    .. code-block:: python\n\n        embedding_size = 256\n        model = LinearClass(EMBEDDING_SIZE)\n\n    Forward pass:\n\n    .. code-block:: python\n\n        inputs = torch.randn(3, 4, 768)\n        outputs = model.forward(inputs)\n\n    Attributes:\n    -----------\n    flatten : `torch.nn.Flatten`\n        Flattens input tensors to a 1D tensor.\n    linear_relu_stack : `torch.nn.Sequential`\n        Sequential container for linear layers followed by ReLU activation.\n\n    Methods:\n    --------\n    forward(inputs: `torch.Tensor`) -> `torch.Tensor`:\n        Performs forward pass through the linear network.\n\n        Parameters:\n            inputs : `torch.Tensor`\n                Input tensor to the network.\n\n        Returns:\n            `torch.Tensor`\n                Output tensor from the network.\n    """"""\n\n    def __init__(self, EMBEDDING_SIZE):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, EMBEDDING_SIZE),\n        )\n\n    def forward(self, inputs):\n        if len(inputs.shape) == 1:\n            inputs = inputs.unsqueeze(0)\n        x = self.flatten(inputs)\n        outputs = self.linear_relu_stack(x)\n        return outputs\n"
"def train(epoch, word2vec, training_loader, model, criterion, optimizer, device, SEED):\n    """"""\n    :param epoch: The current epoch number for training.\n    :param word2vec: The word2vec model used for generating negative samples.\n    :param training_loader: The data loader for the training dataset.\n    :param model: The model to be trained.\n    :param criterion: The loss criterion used for calculating the loss.\n    :param optimizer: The optimizer used for training the model.\n    :param device: The device on which the model and data are loaded (e.g. 'cuda' or 'cpu').\n    :return: None\n\n    Trains the provided model using the given parameters for one epoch. The model is put in training mode.\n    The training data is loaded using the training_loader and processed with the provided model.\n    Negative samples are generated using the word2vec model. The loss is calculated using the criterion and optimizer\n    is used to update the model's parameters. Prints the current epoch number and loss every 500 iterations.\n    """"""\n    random.seed(SEED)\n    model.train()\n    for _, data in enumerate(training_loader, 0):\n        inputs = data['encoded_text'].to(device, dtype=torch.float)\n        targets = data['encoded_reaction'].to(device, dtype=torch.float)\n        outputs = model(inputs)\n\n        negative_list = []\n        for x in range(len(inputs)):\n            negative_list.append(word2vec[random.randint(0, len(word2vec.vectors) - 1)])\n        negative = torch.FloatTensor(negative_list).to(device)\n\n        ones = torch.ones(len(inputs)).to(device)\n\n        optimizer.zero_grad()\n        loss1 = criterion(outputs, targets, ones)\n\n        loss2 = criterion(outputs, negative, torch.mul(ones, -1).to(device))\n        loss = loss1 + loss2 #+ loss3\n        if _ % 500 == 0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        loss.backward()\n        optimizer.step()\n"
"# Hyperparameters\ndef train_model(EMBEDDING_SIZE, BATCH_SIZE, EPOCHS, LEARNING_RATE, SEED):\n    """"""\n    :param EMBEDDING_SIZE: The size of the embedding for each word in the model.\n    :param BATCH_SIZE: The number of samples per batch.\n    :param EPOCHS: The number of epochs to train the model for.\n    :param LEARNING_RATE: The learning rate for the optimizer.\n    :return: None\n\n    This method trains a model using the given parameters. It prints out information about the CUDA availability and\n    device, loads the training data, creates the data loader, initializes the encoder model, optimizer,\n    and criterion. It also loads the word vectors and then trains the model for the specified number of epochs.\n    Finally, it saves the trained model to a file.\n    """"""\n    print(""Cuda:"", torch.cuda.is_available())\n    print(""Device:"", torch.cuda.current_device())\n    print(""Device Name:"", torch.cuda.get_device_name(torch.cuda.current_device()))\n    device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n    print(""Loading Data"")\n    training_set = torch.load('train_encoded.pt', map_location=torch.device(device))\n    training_set.set_train(True)\n\n    train_params = {'batch_size': BATCH_SIZE,\n                    'shuffle': True,\n                    'num_workers': 0\n                    }\n\n    training_loader = DataLoader(training_set, **train_params)\n\n    model = LinearClass(EMBEDDING_SIZE)\n    model.to(device)\n\n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.CosineEmbeddingLoss()\n\n    wv = gen.KeyedVectors.load(""word2vec.wordvectors"", mmap='r')\n    print(""Training Model"")\n    for epoch in range(EPOCHS):\n        train(epoch, wv, training_loader, model, criterion, optimizer, device, SEED)\n\n    print(""Done Training Model"")\n    torch.save(model.state_dict(), ""mymodel.pth"")"
"import numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport gensim.models as gen\n\ndef get_tsne_graph(word2vecFile):\n    """"""\n    :param word2vecFile: The file path to the pretrained Word2Vec model.\n    :return: None\n\n    This method generates a t-SNE graph to visualize the word vectors in the Word2Vec model. It loads the Word2Vec\n    model from the given file path, performs t-SNE dimensionality reduction with 2 components and a perplexity\n    value equal to the length of word vectors minus 1. It then plots the t-SNE coordinates and annotates each point\n    with the corresponding word. Finally, it displays the graph.\n    """"""\n    word2vec = gen.KeyedVectors.load(word2vecFile, mmap='r')\n    tsne_model = TSNE(n_components=2, random_state=42, perplexity=len(word2vec.vectors)-1)\n    tsne_vectors = tsne_model.fit_transform(word2vec.vectors)\n\n    plt.figure(figsize=(15, 15))\n    for i in range(len(tsne_vectors)):\n        plt.scatter(tsne_vectors[i, 0], tsne_vectors[i, 1])\n        plt.annotate(word2vec.index_to_key[i],\n                     xy=(tsne_vectors[i, 0], tsne_vectors[i, 1]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()"
"def modify_relations():\n    """"""\n    Prompts the user to enter the names of two persons and their new relationship level.\n    Updates the relationship level between the two persons in the 'relation' dictionary.\n    Persists the updated 'relation' dictionary in a file named 'relation_data.pkl'.\n\n    :return: None\n    """"""\n    person1 = input(""Enter first person's name: "")\n    person2 = input(""Enter second person's name: "")\n    if person1 in guests and person2 in guests:\n        new_relationship = int(input(f""Enter new relationship level (1-5) for {person1} and {person2}: ""))\n        relation[(person1, person2)] = new_relationship\n        with open('relation_data.pkl', 'wb') as f:\n            pickle.dump(relation, f)"
"def display_arrangement_in_circle(arrangement):\n    """"""\n    Display the arrangement in a circular way as a seating chart.\n\n    :param arrangement: A list of guest's names\n    :return: None\n    """"""\n    n = len(arrangement)\n    x_values = []\n    y_values = []\n    labels = []\n\n    angle = 2 * math.pi / n\n    for i in range(n):\n        x = 10 * math.cos(i * angle)\n        y = 10 * math.sin(i * angle)\n        x_values.append(x)\n        y_values.append(y)\n        labels.append(arrangement[i])\n\n    plt.figure(figsize=(10,10))\n    plt.scatter(x_values, y_values)\n    for i, name in enumerate(labels):\n        plt.annotate(name, (x_values[i], y_values[i]), fontsize=20)\n    plt.title(""Guest Seating Arrangement"")\n    plt.xlabel(""X"")\n    plt.ylabel(""Y"")\n    plt.show()"
"def get_total_familiarity(arrangement):\n    """"""\n    Calculate the total familiarity of a given arrangement.\n\n    :param arrangement: A list representing the seating arrangement of guests.\n    :type arrangement: list\n    :return: The total familiarity score of the arrangement.\n    :rtype: int\n    """"""\n    total = 0\n    for i in range(len(arrangement)):\n        total += familiarity[guests.index(arrangement[i]), guests.index(arrangement[(i + 1) % len(arrangement)])]\n    return total\n"
"def simulated_annealing(arrangement, optimize='min', T=5000, T_min=0.01, alpha=0.9):\n    """"""\n    Simulated Annealing\n\n    :param arrangement: The initial arrangement of elements.\n    :param optimize: The optimization criteria. Possible values are 'min' (default) and 'max'.\n    :param T: The initial temperature. Default value is 5000.\n    :param T_min: The minimum temperature. Default value is 0.01.\n    :param alpha: The temperature reduction factor. Default value is 0.9.\n    :return: The optimized arrangement.\n\n    Simulated annealing is a probabilistic optimization algorithm that is used to find the minimum or maximum of a\n    function by gradually reducing the temperature. It is based on the concept * of simulated annealing in\n    metallurgy, where a material is cooled slowly to reduce defects and improve overall structure.\n\n    The `simulated_annealing` method takes an initial arrangement and performs simulated annealing to optimize the\n    arrangement based on a given optimization criteria. During the annealing process, two elements in the\n    arrangement are randomly swapped and the total familiarity of the arrangement is calculated. The arrangement is\n    updated if the total familiarity increases (for maximizing) or decreases (for minimizing), or based on a\n    probability determined by the temperature and the change in familiarity. The temperature is reduced over time\n    using a temperature reduction factor.\n\n    The method runs until the temperature reaches the minimum temperature. The final optimized arrangement is returned.\n\n    Example usage:\n    ```\n    initial_arrangement = [1, 2, 3, 4, 5]\n    optimized_arrangement = simulated_annealing(initial_arrangement)\n    print(optimized_arrangement)\n    ```\n    """"""\n    while T > T_min:\n        new_arrangement = copy.deepcopy(arrangement)\n        i, j = random.sample(range(len(arrangement)), 2)\n        new_arrangement[i], new_arrangement[j] = new_arrangement[j], new_arrangement[i]\n\n        current_familiarity = get_total_familiarity(arrangement)\n        new_familiarity = get_total_familiarity(new_arrangement)\n\n        delta = new_familiarity - current_familiarity\n        try:\n            accept_probability = math.exp(-delta / T)\n        except OverflowError:\n            accept_probability = 0\n\n        if (delta < 0 and optimize == 'min') or (delta > 0 and optimize == 'max') or random.random() < accept_probability:\n            arrangement = new_arrangement\n        T = T * alpha\n    return arrangement"